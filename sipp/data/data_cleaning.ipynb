{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is courtesy of Ricardo Sandoval and is licensed under an MIT license as follows:\n",
    "\n",
    "```\n",
    "Copyright 2024 Ricardo J. Sandoval\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "```\n",
    "\n",
    "\n",
    "# SIPP 2014 panel data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're loading and preprocessing data from the SIPP dataset. We're trying to get it as close as we can to the the Poverty Tracker dataset. In other words, we're going to try to map the variables not related to shocks from the Poverty Tracker dataset to those in the SIPP dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variables we're interested in\n",
    "opm_variables = {\n",
    "    # 'OPM': , # this could be built using the household income-to-poverty ratio\n",
    "    'OPMTHRESH': 'rhpov', # household poverty threshold in this month, excluding Type 2 individuals\n",
    "    'OPM_RATIO': 'thcyincpov'# household income-to-poverty ratio in this calendar year\n",
    "}\n",
    "\n",
    "hardship_variables = {\n",
    "    'LIVING_QUARTERS_TYPE': 'tlivqtr', # type of living quarters\n",
    "    'LIVING_OWNERSHIP': 'etenure', # are ... living quarters owned, rented, or occupied without payment of rent?\n",
    "    'ENERGY_ASSISTANCE': 'eenergy_asst', # did household receive any energy assistance from the governemnt at any\n",
    "    # time during the reference year?\n",
    "    \n",
    "    # could combine the four variables below into one\n",
    "    'FOOD_VOUCHERS': 'efood_type1', # did ... recieve money, vouchers, or certificates to buy groceries or food?\n",
    "    'FOOD_BAGS': 'efood_type2', # did ... recieve bags of groceries or packaged foods?\n",
    "    'FOOD_CHARITY': 'efood_type3', # did ... recieve assistance from a shelter, soup kitchecn, Meals-on-wheels, or\n",
    "    # other charity?\n",
    "    'FOOD_OTH_HELP': 'efood_oth', # did ... receive some other kind of food assistance?\n",
    "    \n",
    "    # change this into separate variables saying what percentage of the year respondents were covered\n",
    "    'SNAP_BMONTH': 'efs_bmonth', # when did SNAP start?\n",
    "    'SNAP_EMONTH': 'efs_emonth', # when did SNAP end?\n",
    "    'WIC_BMONTH': 'ewic_bmonth', # begin month of WIC spell\n",
    "    'WIC_EMONTH': 'ewic_emonth', # end month of WIC spell\n",
    "    \n",
    "    'HOUSE_PAY_ASSISTANCE': 'ehouse_any', # did ... receive assistance to help pay for housing?\n",
    "}\n",
    "\n",
    "health_variables = {\n",
    "    'MEDICARE_D_PAY': 'tsspartdpd', # how much did ... pay for his/her Medicare Part D coverage?\n",
    "    # change this into separate variables saying what percentage of the year respondents were covered\n",
    "    'MEDICARE_BMONTH': 'emc_bmonth', # begin month of Medicare spell\n",
    "    'MEDICARE_EMONTH': 'emc_emonth', # end month of Medicare spell\n",
    "    'MEDICAID_BMONTH': 'emd_bmonth', # begin month of Medicaid spell\n",
    "    'MEDICAID_EMONTH': 'emd_emonth', # end month of Medicaid spell\n",
    "    \n",
    "    'HEALTHDISAB': 'edisabl', # does ... have a physical, mental or other health condition that limits the kind or\n",
    "    # amount of work he/she can do?\n",
    "    'DAYS_SICK': 'tdaysick', # how many days did illness or injury keep ... in bed more than half of the day?\n",
    "    'HOSPITAL_NIGHTS': 'thospnit', # how many nights did ... spend in the hospital\n",
    "    'PRESCRIPTION_MEDS': 'epresdrg', # did ... take any prescription medications?\n",
    "    'VISIT_DENTIST': 'tvisdent', # how many visits has ... made to a dentist or other dental professional?\n",
    "    'VISIT_DOCTOR': 'tvisdoc', # how many times did ... see or talk to a doctor, nurse, or any other type of medical\n",
    "    # provider about his/her health?\n",
    "    'HEALTH_INSURANCE_PREMIUMS': 'thipay', # how much did ... pay for comprehensive health insurance premiums\n",
    "    'HEALTH_OVER_THE_COUNTER_PRODUCTS_PAY': 'totcmdpay', # how much did ...  pay for his/her non-premium medical\n",
    "    # out-of-pocker expenditures for over-the-counter health-related products?\n",
    "    'HEALTH_MEDICAL_CARE_PAY': 'tmdpay', # how much did ... pay for his/her non-premium medical out-of-pocket\n",
    "    # expenditures on medical care\n",
    "    'HEALTH_FLEX_SPENDING_ACC': 'eflexspnd', # does ... have a Flexible Spending account for health expenses?\n",
    "    'NO_INSUR_DENTIST': 'enoindnt', # during the month(s) ... was not covered by any health insurance, did he/she\n",
    "    # go to a dentist or other dental professional?\n",
    "    'NO_INSUR_DOC': 'enoindoc', # during the month(s) ... was not covered by any health insurance, did he/she go\n",
    "    # to a doctor, nurse, or other medical provider?\n",
    "    'HEALTH_HELP_CLINIC': 'enoincln', # ... recieved health care service sin clinic or public health department\n",
    "    'HEALTH_HELP_ER': 'enoiner', # ... received health care services in ER\n",
    "    'HEALTH_HELP_HSP': 'enoinhsp', # ... received health care services in hospital (excluding ER)\n",
    "    'HEALTH_HELP_VA': 'enoinva', # ... received health care services in VA hospital\n",
    "    'HELATH_HELP_DR': 'enoindr', # ... received health care service in Doctor's office\n",
    "    'HEALTH_HELP_DENTIST': 'enoindds', # ... received health care service in Dentist's office\n",
    "    'HEALTH_HELP_OTH': 'enoinoth', # ... received health care serive somplace else\n",
    "    'HEALTH_ILLNESS_OR_INJURY': 'enointrt', # ... received a treatment for an illness or injury\n",
    "    'HEALTH_PREVENTATIVE': 'enoinchk', # ... received any routine or preventative care\n",
    "    'HEALTH_HEARING': 'ehearing', # is ... deaf or does he/she have serious difficulty hearing?\n",
    "    'HEALTH_SEEING': 'eseeing', # is ... blind or does he/she have serious difficulty seeing?\n",
    "    'HEALTH_COGNITIVE': 'ecognit', # does ... have serious difficulty concentrating, remembering, or making decisions?\n",
    "    'HEALTH_AMBULATORY': 'eambulat', # does ... have serious difficulty walking or climbing stairs?\n",
    "    'HEALTH_SELF_CARE': 'eselfcare', # does ... have difficulty dressing or bathing?\n",
    "    'HEALTH_ERRANDS_DIFFICULTY': 'eerrands', # does ... have difficulty doing errands alone?\n",
    "    'HEALTH_FIND_JOB_DIFFICULTY': 'efindjob', # does ... have difficulty finding a job or remaining employed?\n",
    "    'HEALTH_JOB_DIFFICULTY': 'ejobcant', # is ... prevented from working?\n",
    "    'HEALTH_DEVELOPMENT_DELAY': 'eddelay', # does ... have a serious physical or mental condition or a developmental\n",
    "    # delay that limits ordinary activity?\n",
    "    'HEALTH_CORE_DISABILITY': 'rdis', # indicates individual has at least one of six core disability measures\n",
    "    'HEALTH_SUPPLEMENTAL_DISABILITY': 'rdis_alt', # indicates individual has at least one of six core disability\n",
    "    # measures\n",
    "    'EMPLOYER_HEALTH_INSURANCE': 'eempnoesi', # whether employer offered health insurance to any of its employees\n",
    "}\n",
    "\n",
    "demographics_variables = {\n",
    "    'AGE': 'tage', # age as of last birthday\n",
    "    'GENDER': 'esex',\n",
    "#    'RACE': 'erace', # note, this is much less detailed than `TRACE`\n",
    "    'RACE': 'trace', # note, this is much less detailed than `TRACE`    \n",
    "    'EDUCATION': 'eeduc',\n",
    "    'MARITAL_STATUS': 'ems', # is ... currently married, widowed, divorced, separated, or never married?\n",
    "    'CITIZENSHIP_STATUS': 'ecitizen', # is ... a citizen of the United States\n",
    "    'FAMILY_SIZE': 'rfpersons', # number of persons in the family\n",
    "    'NUM_VEHICLES': 'tveh_num', # number of cars, trucks, or vans owned by the household during the reference period\n",
    "    'ORIGIN' : 'eorigin', #Is ... Spanish, Hispanic, or Latino?\n",
    "}\n",
    "\n",
    "income_variables = {\n",
    "    'HOUSEHOLD_INC': 'thtotinc', # sum of all earnings and income received by a household, from all household\n",
    "    # members age 15 and older for each month of the reference year\n",
    "    \n",
    "    # combine the next three into one variable\n",
    "    'ASSISTANCE_INC': 'tptrninc', # the sum of the reported monthly amounts recieved by an individualfrom TANF\n",
    "    # SSI, Pass-through child support payments, General Assistance, or general relief\n",
    "    'WORKER_COMP_INC': 'tpscininc', # the sum of the reported monthly amounts received by an individual from \n",
    "    # VA benefits (except VA pension), workers' compensation, unemployment compensation, or social security\n",
    "    'OTH_INC': 'tpothinc', # a monthly income recode variable, which is the sum of the reported monthly amounts\n",
    "    # received by an individual from other income sources, such as: survivor benefits, retirement benefits, disability\n",
    "    # benefits, foster child care payments, child support payments, alimony payments, lump sum payments, deferred \n",
    "    # payments from prior job, life insurance payments, or miscellaneous income sources\n",
    "    \n",
    "    # combine the next two into one\n",
    "    'INVESTMENT_INC': 'tpprpinc', # the amount of total personal investment and property income during the reference\n",
    "    # year\n",
    "    'INDIVIDUAL_TOTAL_INC': 'tptotinc', # the sum of reported monthly earnings and income amounts recieved by an \n",
    "    # individual during the refernece year\n",
    "    \n",
    "    # combine the next two into one variable\n",
    "    'IRA_AMOUNT': 'tirakeoval', # value of IRA and KEOGH accounts as of hte last day of the reference period\n",
    "    'AMOUNT_401': 'tthr401val', # value of 401k, 403b, 503b, and Thrift Savings Plan accounts as of the last day of\n",
    "    # the reference period\n",
    "    \n",
    "    'RECEIVED_WORK_COMP': 'ewc_any', # did ... receive worker's compensation payments at any time during the\n",
    "    # reference period?\n",
    "}\n",
    "\n",
    "identifier_variables = {\n",
    "    'SSUID': 'ssuid',\n",
    "    'PNUM': 'pnum'\n",
    "}\n",
    "\n",
    "other_assistance_variables = {\n",
    "    # turn into one variable that encodes percentage of year being covered\n",
    "    'TANF_BMONTH': 'etanf_bmonth', # begin month of TANF spell\n",
    "    'TANF_EMONTH': 'etanf_emonth', # end month of TANF spell\n",
    "    \n",
    "    # combine into one variable that tells whether received assistance with transportation\n",
    "    'GAS_VOUCHERS': 'etrans_type1', # did ... receive gas vouchers?\n",
    "    'PUBLIC_TRANSIT_PASSES': 'etrans_type2', # did ... receive bus or subway tokens or passes?\n",
    "    'CAR_HELP': 'etrans_type3', # did ... receive help registering, repairing, or insuring a car?\n",
    "    'RIDE_TO_DOCTOR': 'etrans_type4', # did ... receive ride to a doctor's office or medical appointment\n",
    "    'OTH_TRANSPORTATION_ASSISTANCE': 'etrans_oth', # did ... receive some other kind of transportation assistnace\n",
    "\n",
    "    'UNEMPLOYMENT_COMP': 'eucany', # did ... receive unemployment compensation payments at any time during the \n",
    "    # reference period?\n",
    "    \n",
    "    # combine the three into one variable\n",
    "    'GOVT_UNEMPLOYMENT_COMP': 'tuc1amt', # amount of regular, governemtn-provided unemployment compensation payments\n",
    "    # received in each month of the reference period\n",
    "    'EMPLOYER_UNEMPLOYMENT_COMP': 'tuc2amt', # amount of supplemental employer-provided unemployment compensation\n",
    "    # payments received ine ach month of reference period\n",
    "    'OTH_UNEMPLOYMENT_COMP': 'tuc3amt', # amount of other unemployment compensation payments, including union benefits\n",
    "    # received in each month of reference period\n",
    "    \n",
    "    'VA_BENEFIT_PAYMENTS': 'evaany', # did ... receive any benefit payments from the department of venteans affairs\n",
    "    # at any time during the reference period\n",
    "    \n",
    "    # combine the next fixe into one variable\n",
    "    'VA_BENEFITS_DISABILITY': 'tva1amt', # how much did ... receive in VA benefits from a service connected \n",
    "    # disability this month?\n",
    "    'VA_BENEFITS_PENSION': 'tva2amt', # how much did ... receive in VA benefits from a veternas' pension this month?\n",
    "    'VA_BENEFITS_OTH': 'tva3amt', # how much did ... receive in VA benefit payments from other VA programs this month?\n",
    "    'VA_BENEFITS_GI': 'tva4amt', # how much did ... receive in VA benefit payments from G.I. bill benefits this month?\n",
    "    'VA_BENEFITS_INSURANCE': 'tva5amt', # how much did ... receive in VA benefit payments from insurance proceeds\n",
    "    # this month?\n",
    "    \n",
    "    # combine the next two into one variable\n",
    "    'SOC_SEC_BENEF_SELF': 'esssany', # did ... receive social security benefits for himself/herself at any time during\n",
    "    # the reference period?\n",
    "    'SOC_SEC_BENEF_CHILD': 'esscany', # did ... receive any social security benefit at any time during the reference\n",
    "    # period on behalf of a child(ren)?\n",
    "    \n",
    "    'CHILD_CARE_PAY_ASST': 'epayhelp', # did reference parent receive assistance to pay for child care?\n",
    "    \n",
    "    # combine next 8 into one variable\n",
    "    'RETIR_INCOME_PENSION': 'tret1amt', # how much retirement income did … receive from a pension from a company \n",
    "    # or union including income from a profit sharing plan this month?\n",
    "    'RETIR_INCOME_FEDERAL_SERVICE': 'tret2amt', # how much retirement income did … receive from a Federal \n",
    "    # civil service or other Federal civilian employee pension this month?\n",
    "    'RETIR_INCOME_STATE_GOVT': 'tret3amt', # how much retirement income did … receive from a State government \n",
    "    # pension this month?\n",
    "    'RETIR_INCOME_LOCAL_GOVT': 'tret4amt', # how much retirement income did … receive from a local government \n",
    "    # person this month?\n",
    "    'RETIR_INCOME_MILITARY': 'tret5amt', # how much retirement income did … receive from a Military retirement \n",
    "    # pay this month?\n",
    "    'RETIR_INCOME_RAILROAD': 'tret6amt', # how much retirement income did … receive from a U.S. \n",
    "    # Government railroad retirement this month?\n",
    "    'RETIR_INCOME_NATIONAL_GUARD': 'tret7amt', # how much retirement income did … receive from a National \n",
    "    # Guard or reserve forces retirement this month?\n",
    "    'RETIR_INCOME_OTH': 'tret8amt', # how much retirement income did … receive from other retirement income \n",
    "    # this month?\n",
    "    \n",
    "    # combine the next variables into one\n",
    "    'SURVIVOR_INC_PENSION': 'tsur1amt', # how much survivor income did … receive in pension from a \n",
    "    # company or union including income from a profit-sharing plan during this month?\n",
    "    'SURVIVOR_INC_VETERAN': 'tsur2amt', # how much survivor income did … receive from Veteran’s \n",
    "    # compensation during this month?\n",
    "    'SURVIVOR_INC_FEDERAL': 'tsur3amt', # how much survivor income did … receive form the \n",
    "    # Federal Civil Service or other Federal civilian employee pension during this month?\n",
    "    'SURVIVOR_INC_RAILROAD': 'tsur4amt', # how much survivor income did … receive from the \n",
    "    # U.S. Governemnt Railroad retirement during this month?\n",
    "    'SURVIVOR_INC_STATE_GOVT': 'tsur5amt', # how much survivor income did … receive from a \n",
    "    # state government during this month?\n",
    "    'SURVIVOR_INC_LOCAL_GOVT': 'tsur6amt', # how much survivor income did … receive from a \n",
    "    # local government pension during this month?\n",
    "    'SURVIVOR_INC_LIFE_INSUR': 'tsur7amt', # how much survivor income did … receive from a \n",
    "    # paid-up life insurance policy or annuity during this month?\n",
    "    'SURVIVOR_INC_MILITARY': 'tsur8amt', # how much survivor income did … receive from military \n",
    "    # retirement pay during this month?\n",
    "    'SURVIVOR_INC_ESTATE': 'tsur11amt', # how much survivor income did … receive from an estate \n",
    "    # or trust during this month?\n",
    "    'SURVIVOR_INC_OTH': 'tsur13amt', # how much other survivor income did … receive during this month?\n",
    "\n",
    "    # combine the next variables into one\n",
    "    'DISABILITY_PAY_INSURANCE': 'tdis1amt', # how much did … receive in payments from a sickness, accident, \n",
    "    # or disability insurance policy this month?\n",
    "    'DISABILITY_PAY_EMPLOYER': 'tdis2amt', # how much did … receive in employer disability payments this month?\n",
    "    'DISABILITY_PAY_PENSION': 'tdis3amt', # how much did … receive in disability income from a pension from a company \n",
    "    # or union including from a profit-sharing plan this month?\n",
    "    'DISABILIITY_PAY_FEDERAL': 'tdis4amt', # how much did … receive in Federal civil service or other federal \n",
    "    # civilian employee pension this month?\n",
    "    'DISABILITY_PAY_STATE': 'tdis5amt', # how much did … receive in disability income from a state government \n",
    "    # pension this month?\n",
    "    'DISABILITY_PAY_LOCAL': 'tdis6amt', #  how much did … receive in disability income from a local government \n",
    "    # pension this month?\n",
    "    'DISABILITY_PAY_MILITARY': 'tdis7amt', # how much did … receive in disability income from a military retirement \n",
    "    # person this month?\n",
    "    'DISABILITY_PAY_OTH': 'tdis10amt', # how much did … receive in other disability income this month?\n",
    "    \n",
    "    'EVER_RETIRE': 'eeveret', # did ... ever retire for any reason from a job or business?\n",
    "    'SEVERANCE_PAY_PENSION': 'elmpnow', # did … receive any severance pay or lump sum payments from a pension \n",
    "    # or retirement plan during the reference period?\n",
    "    'TOTAL_PENSION_LUMP_SUMP_PAY': 'tlmpamt', # what was the total amount of lump sum payments … received during \n",
    "    # the reference period from a pension or retirement plan, or severance pay, or some other type of lump sum \n",
    "    # payment?\n",
    "    'INVEST_ROLL_OVER': 'erollovr1', # did … re-invest or roll over the lump sum payment into an IRA or other \n",
    "    # retirement plan?\n",
    "    'PLAN_INVEST_ROLL_OVER': 'erollovr2', # does … plan on re-inventing or rolling over any of the lump sum \n",
    "    # payment into an IRA or other retirement plan?\n",
    "    'ROLL_OVER_AMT': 'trollamt', # how much did … roll over or plan to roll over into another retirement account?\n",
    "    'DEFERRED_INC_AMT': 'tdeferamt', # what was the total amount of deferred income or final pay-check received?\n",
    "    'TOTAL_LIFE_INSURANCE_AMT': 'tlifeamt', # what was the total amount of life insurance payments … received?\n",
    "    'FOSTER_CHILD_CARE_AMT': 'tfccamt', # amount of foster child care payments received in each month of reference \n",
    "    # period.\n",
    "    'CHILD_SUPPORT_AMT': 'tcsamt', # amount of child support payments in each month of reference period.\n",
    "    'ALIMONY_AMT': 'taliamt', # amount of alimony payments received in each month of reference period\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of the dictionaries of the different variables in order to make it easier to extract them\n",
    "variables_dicts_list = [identifier_variables, opm_variables, hardship_variables, health_variables, \n",
    "                        demographics_variables, income_variables, other_assistance_variables]\n",
    "\n",
    "# create two lists, one with the original SIPP variable names and another one with the names I'll be assigning them\n",
    "sipp_variables_names = [list(vars_dict.values()) for vars_dict in variables_dicts_list]\n",
    "sipp_variables_names = list(chain.from_iterable(sipp_variables_names))\n",
    "\n",
    "my_variables_names = [chain.from_iterable(list(vars_dict.keys())) for vars_dict in variables_dicts_list]\n",
    "my_variables_names = list(chain.from_iterable(my_variables_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_RAW_DATA = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBTAIN THESE FILES FROM THE US CENSUS WEB SITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {\n",
    "    'wave_1': pd.read_stata(PATH_TO_RAW_DATA + 'pu2014w1_v13.dta', columns=sipp_variables_names, preserve_dtypes=True),\n",
    "    'wave_2': pd.read_stata(PATH_TO_RAW_DATA + 'pu2014w2_v13.dta', columns=sipp_variables_names, preserve_dtypes=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_uniqueness_of_column(df, variable_name, exclude_nan=True, print_same_throughout_info=True,\n",
    "                                   return_variating_variable_name=False):\n",
    "    \"\"\"\n",
    "    Determine whether all the entries (for each respondent) for a variable are the same.\n",
    "    \"\"\"\n",
    "    same_throughout = True\n",
    "    for index, data in df[variable_name].iterrows():\n",
    "        if exclude_nan:\n",
    "            data_copy = data.dropna()\n",
    "        else:\n",
    "            data_copy = data.copy()\n",
    "            \n",
    "        if len(data_copy.unique()) > 1:\n",
    "            print(f\"Values in `{variable_name}` ARE NOT the same throughout.\")\n",
    "            same_throughout = False\n",
    "            break\n",
    "    \n",
    "    if same_throughout and print_same_throughout_info:\n",
    "        print(f\"Values in `{variable_name}` ARE the same throghout.\")\n",
    "    \n",
    "    if return_variating_variable_name and not same_throughout:\n",
    "        return variable_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a unique ID for each individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the SIPP documentation for the 2014 panel, each person can be uniquely identified by the combination of <code>ssuid</code> and <code>pnum</code>. Hence, we'll be creatin the <code>UNIQUE_ID</code> by concatenating <code>ssuid</code> + <code>pnum</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_id(df):\n",
    "    \"\"\"\n",
    "    Create a new dataframe that contains the `UNIQUE_ID` column.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # convert the pnum column from an int to a string\n",
    "    df_copy['pnum'] = df_copy['pnum'].astype('str')\n",
    "    \n",
    "    # create the `UNIQUE_ID` column\n",
    "    df_copy['UNIQUE_ID'] = df_copy['ssuid'] + df_copy['pnum']\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data['wave_1'] = create_unique_id(raw_data['wave_1'])\n",
    "preprocessed_data['wave_2'] = create_unique_id(raw_data['wave_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 72919 unique individuals in wave 1.\n",
      "There are 57570 unique individuals in wave 2.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(preprocessed_data['wave_1']['UNIQUE_ID'].unique())} unique individuals in wave 1.\")\n",
    "print(f\"There are {len(preprocessed_data['wave_2']['UNIQUE_ID'].unique())} unique individuals in wave 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way the SIPP data is stored is that each respondent has 12 rows associated to him/her. Each of these rows represents a month within that wave. Hence, we'll be reshaping the dataframe such that there is only one row per unique individual, which will make the rest of our pre-processing much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_df(df):\n",
    "    \"\"\"\n",
    "    Create a new dataframe whose rows are unique individuals in the dataset.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    df_copy['id'] = df_copy.groupby('UNIQUE_ID').cumcount()\n",
    "    return df_copy.pivot(index='UNIQUE_ID', columns='id').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data['wave_1'] = reshape_df(preprocessed_data['wave_1'])\n",
    "preprocessed_data['wave_2'] = reshape_df(preprocessed_data['wave_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out respondents < 18 years old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_underage_individuals(df):\n",
    "    \"\"\"\n",
    "    Drop resondents who are under 18 years of age.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    underage_individuals_indices = df_copy[df_copy['tage'][0] < 18].index\n",
    "    return df_copy.drop(index=underage_individuals_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in `tage` ARE the same throghout.\n"
     ]
    }
   ],
   "source": [
    "determine_uniqueness_of_column(preprocessed_data['wave_1'], 'tage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in `tage` ARE the same throghout.\n"
     ]
    }
   ],
   "source": [
    "determine_uniqueness_of_column(preprocessed_data['wave_2'], 'tage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing valus in col 0 of `tage` for wave 1: 0\n",
      "Number of missing valus in col 0 of `tage` for wave 2: 0\n"
     ]
    }
   ],
   "source": [
    "# determine whether the first column of `tage` has any nan values\n",
    "print(f\"Number of missing valus in col 0 of `tage` for wave 1: {preprocessed_data['wave_1']['tage'][0].isna().sum()}\")\n",
    "print(f\"Number of missing valus in col 0 of `tage` for wave 2: {preprocessed_data['wave_2']['tage'][0].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessed_data['wave_1'] = drop_underage_individuals(preprocessed_data['wave_1'])\n",
    "preprocessed_data['wave_2'] = drop_underage_individuals(preprocessed_data['wave_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the indices for each of the dataframes to prevent any downstream misaligntment of series based on indices\n",
    "preprocessed_data['wave_1'] = preprocessed_data['wave_1'].reset_index(drop=True)\n",
    "preprocessed_data['wave_2'] = preprocessed_data['wave_2'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn monthly variables into yearly variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the variables in the SIPP dataset are taken each month within each wave. To keep the dataframe relatively small (i.e., get rid of 12 * |columns| and turn into |columns|) we are going to be aggregating those variables that have to do with amount of aid recieved, money paid, or coverage received each month. We don't expect to lose that much information doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # see which variables \"fluctuate\" across the 12 months in wave 1\n",
    "# variating_variables_names_w1 = [var\n",
    "#                                 for var in sipp_variables_names if determine_uniqueness_of_column(\n",
    "#                                     preprocessed_data['wave_1'], var, print_same_throughout_info=False,\n",
    "#                                     return_variating_variable_name=True) is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variating_variables_names_w2 = [var\n",
    "#                                 for var in sipp_variables_names if determine_uniqueness_of_column(\n",
    "#                                     preprocessed_data['wave_2'], var, print_same_throughout_info=False,\n",
    "#                                     return_variating_variable_name=True) is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_variating_variables = list(set(variating_variables_names_w1 + variating_variables_names_w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(all_variating_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "variating_vars_by_type = {\n",
    "    'start_stop_percentage': [('efs_bmonth', 'efs_emonth', 'efs'), ('emc_bmonth', 'emc_emonth', 'emc'), \n",
    "                              ('emd_bmonth', 'emd_emonth', 'emd'), ('etanf_bmonth', 'etanf_emonth', 'etanf'), \n",
    "                              ('ewic_bmonth', 'ewic_emonth', 'ewic')],\n",
    "    'sum_across_year': ['taliamt', 'tcsamt', 'tdis10amt', 'tdis1amt', 'tdis2amt', 'tdis3amt', 'tdis4amt', 'tdis5amt',\n",
    "                        'tdis6amt', 'tdis7amt', 'tfccamt', 'thtotinc', 'tpothinc', 'tpscininc', 'tptotinc', \n",
    "                        'tptrninc', 'tret1amt', 'tret2amt', 'tret3amt', 'tret4amt', 'tret5amt', 'tret6amt', \n",
    "                        'tret7amt', 'tret8amt', 'tsur11amt', 'tsur13amt', 'tsur1amt', 'tsur2amt', 'tsur3amt', \n",
    "                        'tsur4amt', 'tsur5amt', 'tsur6amt', 'tsur7amt', 'tsur8amt', 'tuc1amt', 'tuc2amt', 'tuc3amt',\n",
    "                        'tva1amt', 'tva2amt', 'tva3amt', 'tva4amt', 'rhpov'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average across the year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the number of people in the family can change throughout the year, we'll be taking the rounded average of <code>rfpersons</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data['wave_1']['rfpersons'] = round(preprocessed_data['wave_1']['rfpersons'].mean(axis=1))\n",
    "preprocessed_data['wave_2']['rfpersons'] = round(preprocessed_data['wave_2']['rfpersons'].mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage of year covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll be creating a variable that has to do with the percentage of the year that a person was covered/received assistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yearly_percentage(df, bmonth_variable_name, emonth_variable_name, new_variable_name):\n",
    "    num_months = 12\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    yearly_percentage_results = list()\n",
    "    \n",
    "    for bmonth_data, emonth_data in zip(df_copy[bmonth_variable_name].iterrows(), \n",
    "                                        df_copy[emonth_variable_name].iterrows()):\n",
    "        bmonth_values, emonth_values = bmonth_data[1], emonth_data[1]\n",
    "        \n",
    "        # drop the NaN values so that the .unique() function returns an empty list if the row only contains NaN\n",
    "        bmonth_values = bmonth_values.dropna()\n",
    "        emonth_values = emonth_values.dropna()\n",
    "        \n",
    "        bmonth_unique = bmonth_values.unique()\n",
    "        emonth_unique = emonth_values.unique()\n",
    "        \n",
    "        len_bmonth_unique = len(bmonth_unique)\n",
    "        len_emonth_unique = len(emonth_unique)\n",
    "\n",
    "        if len_bmonth_unique == 0 or len_emonth_unique == 0:\n",
    "            # if either of the series is empty, then we're just going to append a 0 because we're assuming there\n",
    "            # was an error with the data (in the case one is empty and the other one isn't) or it is the case that\n",
    "            # the individual did not receive any aid during the current wave\n",
    "            yearly_percentage_results.append(0)\n",
    "        \n",
    "        else:\n",
    "            # compute the percenteage of the year a person was covered. This involves subtracting the\n",
    "            # end month values from the start months values and summing up those differences. We are doing this\n",
    "            # because individuals can stop receiving aid at any point during the year and can strat receiving it\n",
    "            # again at any time\n",
    "            coverage_sum = 0\n",
    "            for begin_val, end_val in zip(bmonth_unique, emonth_unique):\n",
    "                # adding 1 to compensate for the fact that begin_val encodes the month in which each started\n",
    "                # (i.e., if it starts at 1 and goes all the way to 12, we would say that the person was covered\n",
    "                #  100% of the time this year)\n",
    "                coverage_sum += (end_val - begin_val) + 1\n",
    "            yearly_percentage_results.append(round(coverage_sum / num_months, 2))\n",
    "            \n",
    "    df_copy[new_variable_name] = pd.Series(yearly_percentage_results, index=df_copy.index)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efs: 13 unique values\n",
      "emc: 13 unique values\n",
      "emd: 13 unique values\n",
      "etanf: 13 unique values\n",
      "ewic: 13 unique values\n"
     ]
    }
   ],
   "source": [
    "for bmonth_var_name, emonth_var_name, new_var_name in variating_vars_by_type['start_stop_percentage']:\n",
    "    preprocessed_data['wave_1'] = create_yearly_percentage(\n",
    "        preprocessed_data['wave_1'], bmonth_var_name, emonth_var_name, new_var_name)\n",
    "    preprocessed_data['wave_2'] = create_yearly_percentage(\n",
    "        preprocessed_data['wave_2'], bmonth_var_name, emonth_var_name, new_var_name)\n",
    "    \n",
    "    print(f\"{new_var_name}: {len(preprocessed_data['wave_1'][new_var_name].unique())} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum across the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yearly_sum_column(df, variable_name):\n",
    "    \"\"\"\n",
    "    Note: We're treating NaN values as 0 since we're using the sum() function provided by pandas\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # create a series that contains the sum acorss the year for the desired column\n",
    "    df_copy[variable_name] = df_copy[variable_name].sum(axis=1)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in variating_vars_by_type['sum_across_year']:\n",
    "    preprocessed_data['wave_1'] = create_yearly_sum_column(preprocessed_data['wave_1'], var)\n",
    "    preprocessed_data['wave_2'] = create_yearly_sum_column(preprocessed_data['wave_2'], var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variables we're combining\n",
    "variables_to_combine = {\n",
    "    'combine_via_sum': {\n",
    "        'INCOME_FROM_ASSISTANCE': ['tptrninc', 'tpscininc', 'tpothinc'],\n",
    "        'INCOME': ['tpprpinc', 'tptotinc'],\n",
    "        'SAVINGS_INV_AMOUNT': ['tirakeoval', 'tthr401val'],\n",
    "        'UNEMPLOYMENT_COMP_AMOUNT': ['tuc1amt', 'tuc2amt', 'tuc3amt'],\n",
    "        'VA_BENEFITS_AMOUNT': ['tva1amt', 'tva2amt', 'tva3amt', 'tva4amt', 'tva5amt'],\n",
    "        'RETIREMENT_INCOME_AMOUNT': ['tret1amt', 'tret2amt', 'tret3amt', 'tret4amt', 'tret5amt', 'tret6amt', \n",
    "                                     'tret7amt', 'tret8amt'],\n",
    "        'SURVIVOR_INCOME_AMOUNT': ['tsur1amt', 'tsur2amt', 'tsur3amt', 'tsur4amt', 'tsur5amt', 'tsur6amt', 'tsur7amt',\n",
    "                                   'tsur8amt', 'tsur11amt', 'tsur13amt'],\n",
    "        'DISABILITY_BENEFITS_AMOUNT': ['tdis1amt', 'tdis2amt', 'tdis3amt', 'tdis4amt', 'tdis5amt', 'tdis6amt', \n",
    "                                       'tdis7amt', 'tdis10amt']\n",
    "    },\n",
    "    'combine_via_one_hot_encoding': {\n",
    "        'FOOD_ASSISTANCE': ['efood_type1', 'efood_type2', 'efood_type3', 'efood_oth'],\n",
    "        'TRANSPORTATION_ASSISTANCE': ['etrans_type1', 'etrans_type2', 'etrans_type3', 'etrans_type4', 'etrans_oth'],\n",
    "        'SOCIAL_SEC_BENEFITS': ['esssany', 'esscany']\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "# we're creating a new dict with the variables we're preserving so that it's easier for us to assign the new\n",
    "# column names to the new dataframe\n",
    "variables_to_preserve = {\n",
    "    'UNIQUE_ID': 'UNIQUE_ID',\n",
    "    'SSUID': 'ssuid',\n",
    "    'PNUM': 'pnum',\n",
    "    'OPMTHRESH': 'rhpov',\n",
    "    'OPM_RATIO': 'thcyincpov',\n",
    "    'LIVING_QUARTERS_TYPE': 'tlivqtr',\n",
    "    'LIVING_OWNERSHIP': 'etenure',\n",
    "    'ENERGY_ASSISTANCE': 'eenergy_asst',\n",
    "    'SNAP_ASSISTANCE': 'efs', # percentage of year in which individual received assistance from SNAP\n",
    "    'WIC_ASSISTANCE': 'ewic', # percentage of year in which individual received assistance from WIC\n",
    "    'HOUSE_PAY_ASSISTANCE': 'ehouse_any',\n",
    "    'MEDICARE_D_PAY': 'tsspartdpd',\n",
    "    'MEDICARE_ASSISTANCE': 'emc', # percentage of year in which individual received assistance from MEDICARE\n",
    "    'MEDICAID_ASSISTANCE': 'emd', # percentage of year in which individual received assistance from MEDICAID\n",
    "    'HEALTHDISAB': 'edisabl',\n",
    "    'DAYS_SICK': 'tdaysick',\n",
    "    'HOSPITAL_NIGHTS': 'thospnit',\n",
    "    'PRESCRIPTION_MEDS': 'epresdrg',\n",
    "    'VISIT_DENTIST_NUM': 'tvisdent',\n",
    "    'VISIT_DOCTOR_NUM': 'tvisdoc',\n",
    "    'HEALTH_INSURANCE_PREMIUMS': 'thipay',\n",
    "    'HEALTH_OVER_THE_COUNTER_PRODUCTS_PAY': 'totcmdpay',\n",
    "    'HEALTH_MEDICAL_CARE_PAY': 'tmdpay',\n",
    "    'HEALTH_FLEX_SPENDING_ACC': 'eflexspnd',\n",
    "    'NO_INSUR_DENTIST': 'enoindnt',\n",
    "    'NO_INSUR_DOC': 'enoindoc',\n",
    "    'HEALTH_HELP_CLINIC': 'enoincln',\n",
    "    'HEALTH_HELP_ER': 'enoiner',\n",
    "    'HEALTH_HELP_HSP': 'enoinhsp',\n",
    "    'HEALTH_HELP_VA': 'enoinva',\n",
    "    'HELATH_HELP_DR': 'enoindr',\n",
    "    'HEALTH_HELP_DENTIST': 'enoindds',\n",
    "    'HEALTH_HELP_OTH': 'enoinoth',\n",
    "    'HEALTH_ILLNESS_OR_INJURY': 'enointrt',\n",
    "    'HEALTH_PREVENTATIVE': 'enoinchk',\n",
    "    'HEALTH_HEARING': 'ehearing',\n",
    "    'HEALTH_SEEING': 'eseeing',\n",
    "    'HEALTH_COGNITIVE': 'ecognit',\n",
    "    'HEALTH_AMBULATORY': 'eambulat',\n",
    "    'HEALTH_SELF_CARE': 'eselfcare',\n",
    "    'HEALTH_ERRANDS_DIFFICULTY': 'eerrands',\n",
    "    'HEALTH_FIND_JOB_DIFFICULTY': 'efindjob',\n",
    "    'HEALTH_JOB_DIFFICULTY': 'ejobcant',\n",
    "    'HEALTH_DEVELOPMENT_DELAY': 'eddelay',\n",
    "    'HEALTH_CORE_DISABILITY': 'rdis',\n",
    "    'HEALTH_SUPPLEMENTAL_DISABILITY': 'rdis_alt',\n",
    "    'EMPLOYER_HEALTH_INSURANCE': 'eempnoesi',\n",
    "    'AGE': 'tage',\n",
    "    'GENDER': 'esex',\n",
    "    'RACE': 'trace',\n",
    "    'EDUCATION': 'eeduc',\n",
    "    'MARITAL_STATUS': 'ems',\n",
    "    'CITIZENSHIP_STATUS': 'ecitizen',\n",
    "    'FAMILY_SIZE_AVG': 'rfpersons',\n",
    "    'NUM_VEHICLES': 'tveh_num',\n",
    "    'ORIGIN' : 'eorigin', #Is ... Spanish, Hispanic, or Latino?    \n",
    "    'HOUSEHOLD_INC': 'thtotinc',\n",
    "    'RECEIVED_WORK_COMP': 'ewc_any',\n",
    "    'TANF_ASSISTANCE': 'etanf', # percentage of year in which individual received assistance from TANF\n",
    "    'UNEMPLOYMENT_COMP': 'eucany',\n",
    "    'VA_BENEFIT_PAYMENTS': 'evaany',\n",
    "    'CHILD_CARE_PAY_ASST': 'epayhelp',\n",
    "    'EVER_RETIRE': 'eeveret',\n",
    "    'SEVERANCE_PAY_PENSION': 'elmpnow',\n",
    "    'TOTAL_PENSION_LUMP_SUMP_PAY': 'tlmpamt',\n",
    "    'INVEST_ROLL_OVER': 'erollovr1',\n",
    "    'PLAN_INVEST_ROLL_OVER': 'erollovr2',\n",
    "    'ROLL_OVER_AMT': 'trollamt',\n",
    "    'DEFERRED_INC_AMT': 'tdeferamt',\n",
    "    'TOTAL_LIFE_INSURANCE_AMT': 'tlifeamt',\n",
    "    'FOSTER_CHILD_CARE_AMT': 'tfccamt',\n",
    "    'CHILD_SUPPORT_AMT': 'tcsamt',\n",
    "    'ALIMONY_AMT': 'taliamt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_selected_columns(df, selected_columns, new_columns_names):\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    for new_col_name, original_col_name in zip(new_columns_names, selected_columns):\n",
    "        try:\n",
    "            # we'll first check whether we're dealing with a dataframe or just a series. We are going to be doing this\n",
    "            # by trying to access the value at index 1 of the .shape field\n",
    "            num_cols = df[original_col_name].shape[1]\n",
    "            \n",
    "            # if we get here, then we're dealing with a multi-col DataFrame. Hence, we'll go ahead an just retrieve\n",
    "            # the value of the first column\n",
    "            new_df[new_col_name] = df[original_col_name][0].copy()\n",
    "            \n",
    "        except IndexError:\n",
    "            # if an `IndexError` is raised, then it means that we're dealing with a series. Hence, we'll go ahead\n",
    "            # and copy the entire series\n",
    "            new_df[new_col_name] = df[original_col_name].copy()\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframes = {\n",
    "    'wave_1': create_df_from_selected_columns(preprocessed_data['wave_1'], \n",
    "                                              list(variables_to_preserve.values()),\n",
    "                                              list(variables_to_preserve.keys())),\n",
    "    'wave_2': create_df_from_selected_columns(preprocessed_data['wave_2'], \n",
    "                                              list(variables_to_preserve.values()),\n",
    "                                              list(variables_to_preserve.keys())),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_variables_by_adding(df, variables):\n",
    "    vars_combination_sum = None\n",
    "    for i, var in enumerate(variables):\n",
    "        # replace all NaN values with 0\n",
    "        curr_series = df[var][0].copy()\n",
    "        curr_series = curr_series.fillna(0)\n",
    "        \n",
    "        if i == 0:\n",
    "            vars_combination_sum = curr_series.copy()\n",
    "        else:\n",
    "            vars_combination_sum += curr_series.copy()\n",
    "    \n",
    "    return vars_combination_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_variable_name, vars_to_combine in variables_to_combine['combine_via_sum'].items():\n",
    "    final_dataframes['wave_1'][new_variable_name] = combine_variables_by_adding(preprocessed_data['wave_1'],\n",
    "                                                                                vars_to_combine)\n",
    "    final_dataframes['wave_2'][new_variable_name] = combine_variables_by_adding(preprocessed_data['wave_2'],\n",
    "                                                                                vars_to_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_via_one_hot_encoding(df, variables, yes_value=1):\n",
    "    temp_df = pd.DataFrame()\n",
    "    \n",
    "    for var in variables:\n",
    "        temp_df[var] = df[var][0].copy()\n",
    "    \n",
    "    # create a series that only contains zeros and is the size of the dataframe\n",
    "    one_hot_series = pd.Series(np.zeros(len(temp_df)))\n",
    "    \n",
    "    # indices of individuals to whom we'll be assigning a \"yes\" (i.e., a 1)\n",
    "    yes_indices = (temp_df == yes_value).any(axis=1)\n",
    "    \n",
    "    one_hot_series[yes_indices] = 1\n",
    "    \n",
    "    return one_hot_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_variable_name, vars_to_combine in variables_to_combine['combine_via_one_hot_encoding'].items():\n",
    "    final_dataframes['wave_1'][new_variable_name] = combine_via_one_hot_encoding(preprocessed_data['wave_1'],\n",
    "                                                                                 vars_to_combine)\n",
    "    final_dataframes['wave_2'][new_variable_name] = combine_via_one_hot_encoding(preprocessed_data['wave_2'],\n",
    "                                                                                 vars_to_combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop columns with most NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 85 artists>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of NaN values\n",
    "cols_with_nan_w1 = (final_dataframes['wave_1'].isna().sum() / len(final_dataframes['wave_1'])).sort_values(\n",
    "    ascending=False)\n",
    "\n",
    "cols_with_nan_w2 = (final_dataframes['wave_2'].isna().sum() / len(final_dataframes['wave_2'])).sort_values(\n",
    "    ascending=False)\n",
    "\n",
    "x_ticks_w1 = np.arange(len(cols_with_nan_w1))\n",
    "x_ticks_w2 = np.arange(len(cols_with_nan_w2))\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(8, 6))\n",
    "ax[0].bar(x_ticks_w1, cols_with_nan_w1)\n",
    "ax[1].bar(x_ticks_w2, cols_with_nan_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columsn whose NaN values make up > 10% of that column\n",
    "more_than_10_w1 = cols_with_nan_w1[cols_with_nan_w1 > 0.1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are dropping 30 columns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HEALTH_DEVELOPMENT_DELAY       1.000000\n",
       "DEFERRED_INC_AMT               0.999295\n",
       "ROLL_OVER_AMT                  0.997560\n",
       "TOTAL_LIFE_INSURANCE_AMT       0.993277\n",
       "PLAN_INVEST_ROLL_OVER          0.987928\n",
       "INVEST_ROLL_OVER               0.985976\n",
       "TOTAL_PENSION_LUMP_SUMP_PAY    0.982850\n",
       "HEALTH_HELP_VA                 0.927785\n",
       "HEALTH_HELP_CLINIC             0.927785\n",
       "HEALTH_HELP_ER                 0.927785\n",
       "HEALTH_HELP_HSP                0.927785\n",
       "HELATH_HELP_DR                 0.927785\n",
       "HEALTH_HELP_DENTIST            0.927785\n",
       "HEALTH_HELP_OTH                0.927785\n",
       "HEALTH_ILLNESS_OR_INJURY       0.927785\n",
       "HEALTH_PREVENTATIVE            0.927785\n",
       "NO_INSUR_DENTIST               0.922671\n",
       "VA_BENEFIT_PAYMENTS            0.909352\n",
       "MEDICARE_D_PAY                 0.903262\n",
       "NO_INSUR_DOC                   0.900822\n",
       "CHILD_CARE_PAY_ASST            0.881594\n",
       "HEALTH_JOB_DIFFICULTY          0.863161\n",
       "HOWWELL                        0.860161\n",
       "EMPLOYER_HEALTH_INSURANCE      0.694913\n",
       "HEALTH_FLEX_SPENDING_ACC       0.443083\n",
       "HOUSE_PAY_ASSISTANCE           0.408747\n",
       "ENERGY_ASSISTANCE              0.386573\n",
       "EVER_RETIRE                    0.209452\n",
       "NUM_VEHICLES                   0.139496\n",
       "HEALTH_FIND_JOB_DIFFICULTY     0.123719\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the columns we'll be dropping\n",
    "print(f\"We are dropping {len(more_than_10_w1)} columns.\")\n",
    "more_than_10_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll be dropping the same columns for both dataframes\n",
    "final_dataframes['wave_1'] = final_dataframes['wave_1'].drop(columns=more_than_10_w1.index)\n",
    "final_dataframes['wave_2'] = final_dataframes['wave_2'].drop(columns=more_than_10_w1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of respondents left in wave 1: 55335\n",
      "Number of respondents left in wave 2: 44209\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of respondents left in wave 1: {len(final_dataframes['wave_1'])}\")\n",
    "print(f\"Number of respondents left in wave 2: {len(final_dataframes['wave_2'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in wave 1: 0\n",
      "Number of missing values in wave 2: 28466\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of missing values in wave 1: {final_dataframes['wave_1'].isna().sum().sum()}\")\n",
    "print(f\"Number of missing values in wave 2: {final_dataframes['wave_2'].isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of respondents with missing target variable: 1402\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of respondents with missing target variable: {final_dataframes['wave_2']['OPM_RATIO'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNIQUE_ID',\n",
       " 'SSUID',\n",
       " 'PNUM',\n",
       " 'OPMTHRESH',\n",
       " 'OPM_RATIO',\n",
       " 'LIVING_QUARTERS_TYPE',\n",
       " 'LIVING_OWNERSHIP',\n",
       " 'SNAP_ASSISTANCE',\n",
       " 'WIC_ASSISTANCE',\n",
       " 'MEDICARE_ASSISTANCE',\n",
       " 'MEDICAID_ASSISTANCE',\n",
       " 'HEALTHDISAB',\n",
       " 'DAYS_SICK',\n",
       " 'HOSPITAL_NIGHTS',\n",
       " 'PRESCRIPTION_MEDS',\n",
       " 'VISIT_DENTIST_NUM',\n",
       " 'VISIT_DOCTOR_NUM',\n",
       " 'HEALTH_INSURANCE_PREMIUMS',\n",
       " 'HEALTH_OVER_THE_COUNTER_PRODUCTS_PAY',\n",
       " 'HEALTH_MEDICAL_CARE_PAY',\n",
       " 'HEALTH_HEARING',\n",
       " 'HEALTH_SEEING',\n",
       " 'HEALTH_COGNITIVE',\n",
       " 'HEALTH_AMBULATORY',\n",
       " 'HEALTH_SELF_CARE',\n",
       " 'HEALTH_ERRANDS_DIFFICULTY',\n",
       " 'HEALTH_CORE_DISABILITY',\n",
       " 'HEALTH_SUPPLEMENTAL_DISABILITY',\n",
       " 'AGE',\n",
       " 'GENDER',\n",
       " 'RACE',\n",
       " 'EDUCATION',\n",
       " 'MARITAL_STATUS',\n",
       " 'CITIZENSHIP_STATUS',\n",
       " 'FAMILY_SIZE_AVG',\n",
       " 'ORIGIN',\n",
       " 'HOUSEHOLD_INC',\n",
       " 'RECEIVED_WORK_COMP',\n",
       " 'TANF_ASSISTANCE',\n",
       " 'UNEMPLOYMENT_COMP',\n",
       " 'SEVERANCE_PAY_PENSION',\n",
       " 'FOSTER_CHILD_CARE_AMT',\n",
       " 'CHILD_SUPPORT_AMT',\n",
       " 'ALIMONY_AMT',\n",
       " 'INCOME_FROM_ASSISTANCE',\n",
       " 'INCOME',\n",
       " 'SAVINGS_INV_AMOUNT',\n",
       " 'UNEMPLOYMENT_COMP_AMOUNT',\n",
       " 'VA_BENEFITS_AMOUNT',\n",
       " 'RETIREMENT_INCOME_AMOUNT',\n",
       " 'SURVIVOR_INCOME_AMOUNT',\n",
       " 'DISABILITY_BENEFITS_AMOUNT',\n",
       " 'FOOD_ASSISTANCE',\n",
       " 'TRANSPORTATION_ASSISTANCE',\n",
       " 'SOCIAL_SEC_BENEFITS']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the columns that we'll be using\n",
    "final_dataframes['wave_1'].columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop individual with missing target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframes based on their `UNIQUE_ID`\n",
    "final_dataframes['wave_1'] = final_dataframes['wave_1'].sort_values(by='UNIQUE_ID').reset_index(drop=True)\n",
    "final_dataframes['wave_2'] = final_dataframes['wave_2'].sort_values(by='UNIQUE_ID').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondents_to_keep = final_dataframes['wave_2'][~final_dataframes['wave_2']['OPM_RATIO'].isna()]['UNIQUE_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the individual with missing target variables (i.e., `OPM_RATIO` in wave 2) usig their `UNIQUE_ID`\n",
    "final_dataframes['wave_1'] = final_dataframes['wave_1'][final_dataframes['wave_1']['UNIQUE_ID'].isin(\n",
    "    respondents_to_keep)]\n",
    "final_dataframes['wave_2'] = final_dataframes['wave_2'][final_dataframes['wave_2']['UNIQUE_ID'].isin(\n",
    "    respondents_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of respondents left in wave 1: 39720\n",
      "Number of respondents left in wave 2: 42807\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of respondents left in wave 1: {len(final_dataframes['wave_1'])}\")\n",
    "print(f\"Number of respondents left in wave 2: {len(final_dataframes['wave_2'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have more individual in wave 2, we'll drop the ones that are not also included in wave 1\n",
    "wave_1_respondents_ids = final_dataframes['wave_1']['UNIQUE_ID'].copy()\n",
    "\n",
    "final_dataframes['wave_2'] = final_dataframes['wave_2'][final_dataframes['wave_2']['UNIQUE_ID'].isin(\n",
    "    wave_1_respondents_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of respondents left in wave 1: 39720\n",
      "Number of respondents left in wave 2: 39720\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of respondents left in wave 1: {len(final_dataframes['wave_1'])}\")\n",
    "print(f\"Number of respondents left in wave 2: {len(final_dataframes['wave_2'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframes by their `UNIQUE_ID` and reset their indices one last time\n",
    "final_dataframes['wave_1'] = final_dataframes['wave_1'].sort_values(by='UNIQUE_ID').reset_index(drop=True)\n",
    "final_dataframes['wave_2'] = final_dataframes['wave_2'].sort_values(by='UNIQUE_ID').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframes['wave_1']['UNIQUE_ID'].equals(final_dataframes['wave_2']['UNIQUE_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframes['wave_1'].to_csv('sipp_2014_wave_1.csv', \n",
    "                                  index=False)\n",
    "final_dataframes['wave_2'].to_csv('sipp_2014_wave_2.csv', \n",
    "                                  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
